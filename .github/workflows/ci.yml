name: CI
on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    env:
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      AWS_REGION: ${{ secrets.AWS_REGION }}
      AWS_DEFAULT_REGION: ${{ secrets.AWS_REGION }}

    steps:
      - name: Check out code
        uses: actions/checkout@v4
        with:
          submodules: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.9"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
          restore-keys: ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install "dvc[s3]" pytest

      - name: DVC diagnostics
        run: |
          dvc --version
          dvc remote list || true
          cat .dvc/config || true

      - name: Pull & run pipeline
        run: |
          set -euxo pipefail
          dvc pull -v
          dvc checkout -v
          dvc repro -v --force   # produces models/latest and metrics.json

      - name: Gate by metrics (F1 ≥ 0.95)
        run: |
          python - <<'PY'
          import json, sys
          m = json.load(open("metrics.json"))
          f1 = float(m.get("f1_macro", 0))
          print(f"f1_macro = {f1}")
          sys.exit(0 if f1 >= 0.95 else 1)
          PY

      - name: Run tests
        run: pytest -q

      - name: Upload trained model (models/latest)
        uses: actions/upload-artifact@v4
        with:
          name: model-latest
          path: models/latest/

      - name: Upload metrics.json
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: metrics
          path: metrics.json

      - name: Pre-check that model exists before building
        run: |
          ls -la models/latest
          test -f models/latest/MLmodel

      - name: Build Docker image (no cache)
        run: docker build --no-cache -t sentiment-api:${{ github.sha }} .

      - name: Smoke test container
        run: |
          set -euxo pipefail

          # Run in background, map host 8000→container 8000
          CID=$(docker run -d -p 8000:8000 sentiment-api:${{ github.sha }})

          # Wait up to 60s for /health
          for i in $(seq 1 60); do
            if curl -sf http://localhost:8000/health; then
              echo "✅ Service healthy"
              break
            fi
            sleep 1
          done

          # Final health check, or dump logs and fail
          if ! curl -sf http://localhost:8000/health; then
            echo "❌ Service never became healthy; container logs:"
            docker logs "$CID" || true
            exit 1
          fi

          # Hit predict
          curl -sf -X POST http://localhost:8000/predict \
            -H "Content-Type: application/json" \
            -d '{"texts":["great movie","terrible plot"]}'

          # Clean up
          docker stop "$CID"

      - name: Log in to GHCR
        run: echo "${{ secrets.GHCR_TOKEN }}" | docker login ghcr.io -u ${{ secrets.GHCR_USERNAME }} --password-stdin

      - name: Push image to GHCR
        run: |
          IMAGE=ghcr.io/${{ github.repository_owner }}/sentiment-api:${{ github.sha }}
          docker tag sentiment-api:${{ github.sha }} "$IMAGE"
          docker push "$IMAGE"

      - name: Tag latest on main
        if: github.ref == 'refs/heads/main'
        run: |
          IMAGE=ghcr.io/${{ github.repository_owner }}/sentiment-api
          docker tag $IMAGE:${{ github.sha }} $IMAGE:latest
          docker push $IMAGE:latest
